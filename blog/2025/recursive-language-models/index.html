<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Recursive Language Models reduce context rot and 2.5× accuracy on BrowseComp‑Plus (at 2.6× latency) | Blog on AI Eng </title> <meta name="author" content="Byron Tang"> <meta name="description" content="Capturing learnings from papers and experiments. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tangbyron.github.io/blog/2025/recursive-language-models/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Blog on AI Eng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Recursive Language Models reduce context rot and 2.5× accuracy on BrowseComp‑Plus (at 2.6× latency)</h1> <p class="post-meta"> Created on November 09, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/retrieval"> <i class="fa-solid fa-hashtag fa-sm"></i> retrieval</a>   <a href="/blog/tag/multi-hop"> <i class="fa-solid fa-hashtag fa-sm"></i> multi-hop</a>   <a href="/blog/tag/browsecomp"> <i class="fa-solid fa-hashtag fa-sm"></i> browsecomp</a>   ·   <a href="/blog/category/search"> <i class="fa-solid fa-tag fa-sm"></i> Search</a>   <a href="/blog/category/reasoning"> <i class="fa-solid fa-tag fa-sm"></i> Reasoning</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="main-takeaway">Main takeaway</h2> <p>Building off of the great post on <a href="https://alexzhang13.github.io/blog/2025/rlm/" rel="external nofollow noopener" target="_blank">Recursive Language Models</a>, I ran a similar experiment, and found that RLM with sub-LM summarization and reflection achieved a 2.5X increase in accuracy (10% -&gt; 25%, using Gemini Flash 2.5) on <a href="https://arxiv.org/abs/2508.06600" rel="external nofollow noopener" target="_blank">BrowseComp Plus</a>, compared to a ReAct baseline using the same search tool (BM25).</p> <p>The key insight is that we are able to reduce context rot for the root-LM, by delegating document processing, summarization, and reflection to the sub-LM (depth == 1). Instead of the ReAct agent continuously appending documents and growing context at a rapid rate (10 documents per search, truncate to first 512 tokens, 15 iterations = 77K tokens), the sub-LM is able to limit context growth per iteration to ~500 tokens, a 10× reduction that significantly improves its ability to answer the multi-hop research question. Interestingly, document evidence recall was around the same for ReAct and RLM (~32%), further demonstrating that the improvement is from reducing context rot, rather than improving retrieval.</p> <p>Another key factor of reducing context rot is traversing the search more efficiently. I observed a 13% decrease in average number of searches per query from 8.95 in ReAct to 7.75 in RLM. From ablations, the sub‑LM’s reflections on the hypothesis and suggested query refinements were a significant step up from the simple tool outputs the root‑LM sees in ReAct.</p> <p>The downside is that the RLM process is roughly 2.6× slower (per‑query average of 203s vs 79s). Parallelization could help, but it’s not obviously a silver bullet given the sequential nature of multi‑hop research. Future experiments will combine RLM with a Memory Bank to reduce processing time and context.</p> <hr> <h2 id="motivation">Motivation</h2> <p>I wrote about context rot and continual learning in the <a href="https://tangbyron.github.io/posts/anti-patterns-as-guardrails/">previous post</a>, so I won’t belabor the point. I did find that the BrowseComp Plus dataset is a perfect one for exploring this problem, at least on the text modality.</p> <hr> <h2 id="dataset-browsecomp-plus">Dataset: BrowseComp Plus</h2> <p><a href="https://arxiv.org/html/2508.06600v1" rel="external nofollow noopener" target="_blank">BrowseComp Plus</a> (and the original <a href="https://openai.com/index/browsecomp/" rel="external nofollow noopener" target="_blank">BrowseComp</a>) is a multi-hop question answering benchmark designed to stress-test retrieval systems. It contains:</p> <ul> <li> <strong>830 queries</strong> requiring cross-document synthesis</li> <li> <strong>100,195 documents</strong> wide range of document length</li> <li> <strong>~6.1 evidence docs, 2.9 gold docs, and ~76.3 hard negatives</strong> per query</li> </ul> <p>Shoutout to the authors for putting the dataset together, I found it very cool that answers are contained in multiple documents, and requires multi-hop search and reasoning. My initial attempts sent Gemini down deep rabbit holes and I ended up burning some $ on large context API calls. There’s a reason why major model releases like <a href="https://huggingface.co/moonshotai/Kimi-K2-Thinking" rel="external nofollow noopener" target="_blank">Kimi K2 Thinking</a> in Nov 2025 still reference the performance on this dataset.</p> <h3 id="synthetic-example">Synthetic Example</h3> <p>Per the dataset card’s policy (“BENCHMARK DATA SHOULD NEVER APPEAR AS PLAIN TEXT ONLINE” <a href="https://huggingface.co/datasets/Tevatron/browsecomp-plus" rel="external nofollow noopener" target="_blank">HF dataset card</a>), here’s a representative synthetic example (not from the benchmark):</p> <blockquote> <p><strong>query</strong>: “Identify the 2012–2016 sci‑fi short story that won a major award, is set on a tidally‑locked planet, and whose author later chaired the Nebula Awards.”</p> <p><strong>multi-hop challenges</strong>:</p> <ol> <li>Search for award-winning sci-fi stories (2012-2016)</li> <li>Filter by setting (tidally-locked planet)</li> <li>Cross-reference author’s later career (Nebula chair)</li> <li>Synthesize across 3+ documents with different constraints</li> </ol> </blockquote> <hr> <h2 id="baseline-single-llm-call">Baseline: Single LLM call</h2> <p><strong>Setup</strong>: The simplest possible approach. Execute one BM25 search with k=20 documents, truncate each to the first 512 tokens, pass directly to Gemini 2.5 Flash, and ask for the final answer.</p> <p><strong>Results</strong>:</p> <ul> <li><strong>Accuracy: 0% (0/20 correct)</strong></li> <li>Evidence recall: 2%</li> <li>Average time: 118s per query</li> </ul> <p>Interestingly, with BM25 search using k=20, there are times that we retrieved the golden evidence docs. But even so, the model could not synthesize the final answer. Again showcasing the multi-hop nature of this dataset.</p> <hr> <h2 id="experiment-1-react-baseline">Experiment 1: ReAct Baseline</h2> <h3 id="architecture">Architecture</h3> <p>The ReAct pattern (<a href="https://arxiv.org/abs/2210.03629" rel="external nofollow noopener" target="_blank">Yao et al., 2023</a>) enables iterative search through a simple loop: Reason → Act (tool call) → Observe (append results) → Repeat.</p> <pre><code class="language-mermaid">Query → Root LM
         ↓
    ┌────────────┐
    │ Reasoning  │  Think about what to search
    └─────┬──────┘
          ↓
    ┌────────────┐
    │ Tool Call  │  Execute BM25 search
    └─────┬──────┘
          ↓
    ┌────────────┐
    │ Append     │  Add results to conversation
    └─────┬──────┘
          ↓
    Loop back (max 15 iterations)
</code></pre> <h3 id="implementation">Implementation</h3> <p>Kept it simple, no frameworks, just native Gemini SDK function calling in ~20 lines of core logic:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Core ReAct loop
for iteration in range(max_iterations):
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=contents,  # Growing context from appending search results
        config=types.GenerateContentConfig(
            tools=[search_tool],  # BM25 function
            system_instruction=REACT_SYSTEM_PROMPT,
            tool_config=types.ToolConfig(
                function_calling_config=types.FunctionCallingConfig(
                    mode='AUTO'  # Let LLM decide when to call
                )
            )
        )
    )

    if function_calls:
        results = execute_search(...)
        contents.append(...)  # Context accumulates here
        continue
    else:
        return final_answer
</code></pre></div></div> <h3 id="results">Results</h3> <p>Using <strong>Gemini 2.5 Flash</strong> (knowledge cutoff of Jan 2025, pre-dating BrowseComp Plus release to avoid contamination):</p> <ul> <li><strong>Accuracy: 10.00% (2/20 correct)</strong></li> <li>Evidence recall: 32% (15x better than single search!)</li> <li>Average 8.95 searches per query</li> <li>Average 9.8 iterations</li> <li>Average time: 79.25s per query</li> </ul> <p>It was encouraging to see non‑zero accuracy (was worth celebrating at the time)! Somewhat matches the 15.54% reported in the original <a href="https://arxiv.org/abs/2508.06600" rel="external nofollow noopener" target="_blank">BrowseComp Plus</a> paper. I’m sure there was some more prompt tuning for the paper, but since that’s not the primary objective of this experiment, I just went with a generic deep research prompt.</p> <p>I repeated the ReAct baseline a few times, the key issue is the linear context accumulation, since each iteration simply appends the search results to the context:</p> <ul> <li>Iteration 1: 5,120 tokens (10 docs × 512 tokens)</li> <li>Iteration 5: 25,600 tokens</li> <li>Iteration 15 (max): 76,800 tokens</li> </ul> <p>While this is well below the 1 million token context window for Gemini Flash 2.5, I observed that the root-LM was getting bogged down from the ever increasing context, and it would repeat very similar search queries with a few keyword differences, and prematurely “give up” after 10+ iterations.</p> <hr> <h2 id="experiment-2-recursive-language-model">Experiment 2: Recursive Language Model</h2> <h3 id="architecture-1">Architecture</h3> <p>To reduce context rot on the root-LM, we introduce sub-LM at depth == 1</p> <pre><code class="language-mermaid">sequenceDiagram
  participant User
  participant RootLM as Root‑LM
  participant Search as BM25
  participant SubLM as Sub‑LM

  User-&gt;&gt;RootLM: Question
  RootLM-&gt;&gt;Search: Query (k=10)
  Search--&gt;&gt;RootLM: 10×512 tokens per doc
  RootLM-&gt;&gt;SubLM: Hypothesis + docs
  SubLM--&gt;&gt;RootLM: ~500 token summary + reflection
  RootLM-&gt;&gt;Search: Refined query
  loop until confident or budget hit
    Search--&gt;&gt;RootLM: New docs
    RootLM-&gt;&gt;SubLM: Compress &amp; Reflect
  end
  RootLM--&gt;&gt;User: Final answer (+ cited docids)
</code></pre> <p>The sub-LM is responsible for reviewing all of the documents, comparing it against the original query from BrowseComp, the search query and hypothesis from the root-LM. It’s able to summarize the information, reflect, and provide feedback to the root‑LM on how to update its query. It condenses the output to ~500 tokens, which means 10X decrease in tokens for the root-LM, as it iterates through the research process.</p> <h3 id="root-lm-and-sub-lm-interaction">Root-LM and Sub-LM interaction</h3> <p>In addition to the obvious benefits of compression, I found that the root/sub LM interaction to be interesting and beneficial. Specifically, feedback from the sub-LM was critical in introducing new search patterns in the root-LM, which prevented the root-LM from getting “stuck” in a single train of thought.</p> <p>The root-LM articulates its hypothesis every turn using a structured 5-step format:</p> <p><strong>Process (every turn):</strong></p> <ol> <li> <strong>Observations:</strong> Review question, summarize what new signal the latest evidence added.</li> <li> <strong>Evidence Gaps:</strong> List what’s still missing to answer confidently.</li> <li> <strong>Leading Hypothesis:</strong> State your current best hypothesis in one clear sentence.</li> <li> <strong>Search Plan:</strong> Choose your next search query. Include both the query string and the rationale for why it fills the gaps.</li> <li> <strong>Final Answer:</strong> When evidence converges and you’re confident, provide an answer with document IDs and confidence (0-100).</li> </ol> <p>The sub-LM receives this hypothesis and provides feedback:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">build_feedback_prompt_v2</span><span class="p">(...,</span> <span class="n">leading_hypothesis</span><span class="p">):</span>
<span class="sh">"""</span><span class="s">Build hypothesis-aware sub-LM prompt with reflection</span><span class="sh">"""</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"""</span><span class="s">
ORIGINAL QUESTION: </span><span class="si">{</span><span class="n">original_query</span><span class="si">}</span><span class="s">
CURRENT SEARCH: </span><span class="sh">"</span><span class="si">{</span><span class="n">search_query</span><span class="si">}</span><span class="sh">"</span><span class="s">

    LEADING HYPOTHESIS FROM THE ANALYST:
    </span><span class="se">\"\"\"</span><span class="si">{</span><span class="n">leading_hypothesis</span><span class="si">}</span><span class="se">\"\"\"</span><span class="s">

    Your goals:
    1. Review all documents and return key findings summary including:
       - Key facts that support or contradict the hypothesis
       - Detailed relationships and connections between entities

    2. For highly relevant documents, conduct a deep dive into the text.

    3. Suggest how the analyst should update the hypothesis if documents
       provide strong contradictory evidence.

    4. Note any follow-up keywords or entities the analyst should
       consider next.

    After your document analysis, provide:

    **Suggested Next BM25 Queries:**
    - Query 1: </span><span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="s">
    - Query 2: </span><span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="s">
    - Query 3: </span><span class="sh">"</span><span class="s">...</span><span class="sh">"</span><span class="s">
    </span><span class="sh">"""</span></code></pre></figure> <h3 id="results-1">Results</h3> <p>Accuracy of 25%, a roughly 2.5X improvement over ReAct. A huge caveat is that this is only on 20 queries, repeated over multiple runs of the experiment. It’s a bit expensive to run these experiments (already used up about $100 of Gemini credits), and there’s more low hanging fruits to explore in the RLM framework, so I’ll stop at 20 for now.</p> <table> <thead> <tr> <th>Metric</th> <th>Single BM25</th> <th>ReAct</th> <th>RLM</th> <th>Change (ReAct→RLM)</th> </tr> </thead> <tbody> <tr> <td><strong>Accuracy</strong></td> <td>0.00%</td> <td>10.00%</td> <td><strong>25.00%</strong></td> <td><strong>+150%</strong></td> </tr> <tr> <td><strong>Evidence Recall</strong></td> <td>2.08%</td> <td>31.64%</td> <td>32.24%</td> <td>+1.9%</td> </tr> <tr> <td><strong>Avg Searches</strong></td> <td>1</td> <td>8.95</td> <td>7.75</td> <td>-13.4%</td> </tr> <tr> <td><strong>Avg Confidence</strong></td> <td>-</td> <td>39.75%</td> <td>51.25%</td> <td>+28.9%</td> </tr> <tr> <td><strong>Avg Time (s)</strong></td> <td>118</td> <td>79</td> <td>203</td> <td>+156.8%</td> </tr> </tbody> </table> <p>It was great to see the accuracy increase, and very interesting that it wasn’t due to evidence recall, which was roughly about the same between ReAct and RLM. Rather, the context reduction from summarization, and also getting to the evidence with fewer iterations (-13%), means a 10X+ reduction in context for the root-LM, which significantly helped with accuracy.</p> <hr> <h2 id="limitations--future-work">Limitations &amp; Future Work</h2> <p>Small sample size is the obvious one, I’ll increase this in the near future, once we explore the following augmentations to RLM:</p> <ol> <li>for the apples to apples comparison between ReAct and RLM, we are just using the BM25 search tool. Adding Regex, Word Search, and Semantic search will be fast follows</li> <li>for now, the tools are static, what if we allow the root-LM dynamically create tools (code execution) that spin up sub-LMs? what if we allow depth &gt; 1, and the sub-LMs can then dynamically create and execute additional actions?</li> <li>As mentioned in the <a href="https://tangbyron.github.io/posts/anti-patterns-as-guardrails/">previous post</a>, as we give more freedom to the RLM to dynamically execute, it’ll surely come upon happy/unhappy paths. What if we encode these as memory, and allow the RLM to also dynamically retrieve from this memory bank for past reasoning/execution traces?</li> </ol> <hr> <p><em>This work was inspired by <a href="https://alexzhang13.github.io/blog/2025/rlm/" rel="external nofollow noopener" target="_blank">Recursive Language Models</a> by Alex Zhang and builds on the <a href="https://arxiv.org/html/2508.06600v1" rel="external nofollow noopener" target="_blank">BrowseComp Plus dataset</a>.</em></p> <p><em>Views are strictly my own. Experiments based only on public datasets.</em></p> <p><em>Published: November 9, 2025</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/context-engineering-in-practice/">Context Engineering for Agents: Three Levels of Disclosure</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/recursive-lm-code-execution/">Recursive Language Models + Code Execution: 60% accuracy on BrowseComp Plus (no embeddings)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anti-patterns-as-guardrails/">Exploring Continuous Learning: Reasoning Bank + Recursive Language Models</a> </li> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>