<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Context Engineering for Agents: Three Levels of Disclosure | Blog on AI Eng </title> <meta name="author" content="Byron Tang"> <meta name="description" content="Capturing learnings from papers and experiments. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tangbyron.github.io/blog/2025/context-engineering-in-practice/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Blog on AI Eng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/"> </a> </li> <li class="nav-item "> <a class="nav-link" href="/about/">about </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Context Engineering for Agents: Three Levels of Disclosure</h1> <p class="post-meta"> Created on December 08, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/context-engineering"> <i class="fa-solid fa-hashtag fa-sm"></i> context-engineering</a>   <a href="/blog/tag/claude"> <i class="fa-solid fa-hashtag fa-sm"></i> claude</a>   <a href="/blog/tag/production"> <i class="fa-solid fa-hashtag fa-sm"></i> production</a>   ·   <a href="/blog/category/agents"> <i class="fa-solid fa-tag fa-sm"></i> Agents</a>   <a href="/blog/category/context"> <i class="fa-solid fa-tag fa-sm"></i> Context</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="main-takeaway">Main takeaway</h2> <p>Sharing a few tips from implementing a tool-heavy agent that operates on a lot of data using Claude Agent SDK:</p> <ol> <li>shield the primary agent using both client-side filtering and server-side context editing (Anthropic’s <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">context_management</code></a>).</li> <li>design for three levels of disclosure, from least to most: 1) the primary agent, 2) user, 3) logs. Managing all three are important to allow for efficiency (agent), transparency (user), and faster iteration cycles (logs).</li> <li>use the <a href="https://docs.anthropic.com/en/docs/build-with-claude/token-counting" rel="external nofollow noopener" target="_blank">token count API</a> to estimate the # of tokens that would be sent to the primary agent. Print out the progress bar as you iterate, and it doubles as visual feedback to the user on how much context they’ve used.</li> </ol> <hr> <h2 id="motivation">Motivation</h2> <p>I’ve been exploring <a href="https://research.trychroma.com/context-rot" rel="external nofollow noopener" target="_blank">context rot</a> in prior posts—from <a href="https://tangbyron.github.io/posts/anti-patterns-as-guardrails/">Reasoning Banks</a> to <a href="https://tangbyron.github.io/posts/recursive-language-models/">Recursive Language Models</a> to <a href="https://tangbyron.github.io/posts/recursive-lm-code-execution/">RLM + Code Execution</a>. Those experiments used research datasets like BrowseComp Plus where I could control context growth.</p> <p>Production is different. First, a shoutout to Claude Haiku (and yes of course Opus 4.5 is incredible). For low latency use cases where users are iterating in the flow, I’ve found Haiku 4.5 to be quite amazing at tool calling, code execution, and understanding structured data like spreadsheets. Haiku “only” has 200K tokens, and I’ve found that single tool calls like web search or complex chart generation could cause <code class="language-plaintext highlighter-rouge">ERROR: prompt is too long: 208991 tokens &gt; 200000 maximum</code>.</p> <hr> <h2 id="managing-context">Managing Context</h2> <p>The goal is to ensure the primary agent only remembers what would be helpful to reason about the next step. Aggressive filtering is key here. It’s also important to remember that there are two important other parties in the system:</p> <ul> <li>the user wants transparency on what’s being done, and also see outputs like charts or forecasts</li> <li>logs are also critical as you iterate on adding more tools and expanding the use case. This is especially true when asking the primary agent to leverage <a href="https://www.anthropic.com/engineering/code-execution-with-mcp" rel="external nofollow noopener" target="_blank">code execution</a> to chain multiple tool calls. Reviewing the logs of errors is key to refining the sandbox env, tool descriptions, and system prompt.</li> </ul> <p><img src="/assets/img/posts/2025-12-08-context-three-levels-disclosure.jpg" alt="Three levels of disclosure diagram"></p> <hr> <h2 id="shielding-the-primary-agent">Shielding the Primary Agent</h2> <h3 id="layer-1-server-side-context-management">Layer 1: Server-Side Context Management</h3> <p>Anthropic provides an API for automatic cleanup: the <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">context_management</code></a> parameter (beta). This clears old tool calls when context goes over a certain limit.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">with</span> <span class="n">client</span><span class="p">.</span><span class="n">beta</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">stream</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">max_tokens</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">system</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">context-management-2025-06-27</span><span class="sh">"</span><span class="p">],</span>
    <span class="n">context_management</span><span class="o">=</span><span class="p">{</span>
        <span class="sh">"</span><span class="s">edits</span><span class="sh">"</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">clear_tool_uses_20250919</span><span class="sh">"</span><span class="p">,</span>
                <span class="c1"># Trigger at ~50% capacity to leave headroom
</span>                <span class="sh">"</span><span class="s">trigger</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">input_tokens</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">value</span><span class="sh">"</span><span class="p">:</span> <span class="mi">100000</span><span class="p">},</span>
                <span class="sh">"</span><span class="s">keep</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">tool_uses</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">value</span><span class="sh">"</span><span class="p">:</span> <span class="mi">2</span><span class="p">},</span>
                <span class="sh">"</span><span class="s">clear_at_least</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span><span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">input_tokens</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">value</span><span class="sh">"</span><span class="p">:</span> <span class="mi">15000</span><span class="p">},</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">)</span> <span class="k">as</span> <span class="n">stream</span><span class="p">:</span>
</code></pre></div></div> <p>I’m triggering at ~50% of capacity here to leave headroom. If your tools can return very large payloads in a single turn (e.g., 100K+ tokens of web search results), you may want to trigger earlier.</p> <p>When the trigger fires, old tool results are replaced with a placeholder:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Tool result cleared to manage context length]
</code></pre></div></div> <p>Claude understands this. If it needs that data again, it can re-call the tool.</p> <h3 id="layer-2-client-side-filtering">Layer 2: Client-Side Filtering</h3> <p>Server-side clearing only helps if your request is <em>already under</em> 200K tokens. For tools that produce massive output, you need client-side filtering before adding to conversation history. You could also enable client-side <a href="https://platform.claude.com/docs/en/build-with-claude/context-editing#using-compaction" rel="external nofollow noopener" target="_blank">Compaction</a>, which summarizes and replaces the conversation history. That’s super useful as a coarse control that’s intentionally lossy, and operates at session level. For tools that have known large results, I’d prefer to apply the methods below:</p> <table> <thead> <tr> <th>Scenario</th> <th>Approach</th> <th>When to Use</th> <th>Example</th> </tr> </thead> <tbody> <tr> <td>Web search returns 100K+ tokens</td> <td><strong>Sub-agent summarization</strong></td> <td>Content has semantic meaning agent needs to reason about</td> <td>Spawn sub-agent to synthesize into 2-3 sentences</td> </tr> <tr> <td>Chart generation outputs base64</td> <td><strong>Simple filtering</strong></td> <td>Content is binary/visual; agent just needs confirmation</td> <td>Replace with <code class="language-plaintext highlighter-rouge">[CHART_GENERATED]</code> </td> </tr> <tr> <td>Code writes CSV to disk</td> <td><strong>Pointer storage</strong></td> <td>Content is regenerable; agent can reload on demand</td> <td>Store <code class="language-plaintext highlighter-rouge">file_id</code> only, fetch if needed</td> </tr> </tbody> </table> <h4 id="web-search">Web search</h4> <p>For <a href="https://platform.claude.com/docs/en/agents-and-tools/tool-use/web-search-tool" rel="external nofollow noopener" target="_blank">web search</a>, I use another LLM with isolated context to compress 100K+ tokens down to 2-3 sentences. This pattern is described in detailed in my <a href="https://tangbyron.github.io/blog/2025/recursive-lm-code-execution/">Recursive Language Model</a> post.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">async</span> <span class="k">def</span> <span class="nf">execute_web_research</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="c1"># spawn separate llm call (isolated context)
</span>    <span class="n">response</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">claude-haiku-4-5</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>  <span class="c1"># can force compact output
</span>        <span class="n">system</span><span class="o">=</span><span class="sh">"</span><span class="s">Synthesize web search results into 300-500 words</span><span class="sh">"</span><span class="p">,</span> <span class="c1"># replace with your synthesis prompt
</span>        <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">WEB_SEARCH_TOOL</span><span class="p">],</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">query</span><span class="p">}],</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">summary</span><span class="sh">"</span><span class="p">:</span> <span class="nf">extract_text</span><span class="p">(</span><span class="n">response</span><span class="p">),</span> <span class="sh">"</span><span class="s">sources</span><span class="sh">"</span><span class="p">:</span> <span class="n">urls</span><span class="p">}</span>
</code></pre></div></div> <h4 id="visualizations">Visualizations</h4> <p>For charts generated from code execution, the three levels of disclosure work like this: the sandbox saves the PNG to a temp file and returns base64. The <strong>UI</strong> streams the full image to the user. The <strong>agent</strong> only sees <code class="language-plaintext highlighter-rouge">[CHART_GENERATED]</code>. The <strong>logs</strong> capture the full payload for debugging.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_filter_tool_result_for_history</span><span class="p">(</span><span class="n">tool_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">result</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">tool_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">execute_code</span><span class="sh">'</span> <span class="ow">and</span> <span class="n">result</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">success</span><span class="sh">'</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">success</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">output</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">chart</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">[CHART_GENERATED]</span><span class="sh">'</span><span class="p">},</span>
            <span class="c1"># Omit: stdout, stderr, base64 – these go to UI/logs, not the agent’s context
</span>        <span class="p">}</span>
</code></pre></div></div> <p>I actually hit a tricky bug on this. I had this filtering but still hit overflow after chart generation. Turns out the sandbox printed the entire output dict (including base64) to stdout for IPC—so there were <em>two copies</em> of the chart data. Make sure to remove stdout from agent context entirely (307KB → 59 bytes). Stdout still goes to logs.</p> <hr> <h2 id="tip-token-observability">Tip: Token Observability</h2> <p>Tracking tokens in memory is critical to optimizing the overall agent. Two ways to track it:</p> <table> <thead> <tr> <th>Method</th> <th>When to Use</th> </tr> </thead> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">client.messages.count_tokens()</code></td> <td>Before API call</td> </tr> <tr> <td><code class="language-plaintext highlighter-rouge">response.usage.input_tokens</code></td> <td>After API call</td> </tr> </tbody> </table> <p>The <a href="https://docs.anthropic.com/en/docs/build-with-claude/token-counting" rel="external nofollow noopener" target="_blank">Token Counting API</a> mentioned that its an estimate, but I’ve found it to be very accurate. The pre-flight counting detects overflow before it happens:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">token_count</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="p">.</span><span class="n">messages</span><span class="p">.</span><span class="nf">count_tokens</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">system</span><span class="o">=</span><span class="n">system</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span> <span class="n">messages</span><span class="o">=</span><span class="n">messages</span>
<span class="p">)</span>
<span class="n">usage_pct</span> <span class="o">=</span> <span class="n">token_count</span><span class="p">.</span><span class="n">input_tokens</span> <span class="o">/</span> <span class="mi">200000</span>
<span class="n">warning</span> <span class="o">=</span> <span class="sh">"</span><span class="s">critical</span><span class="sh">"</span> <span class="k">if</span> <span class="n">usage_pct</span> <span class="o">&gt;=</span> <span class="mf">0.9</span> <span class="k">else</span> <span class="sh">"</span><span class="s">warning</span><span class="sh">"</span> <span class="k">if</span> <span class="n">usage_pct</span> <span class="o">&gt;=</span> <span class="mf">0.75</span> <span class="k">else</span> <span class="sh">"</span><span class="s">normal</span><span class="sh">"</span>
</code></pre></div></div> <hr> <h2 id="results">Results</h2> <p>Here’s a realistic scenario: the user asks<br> <em>“Analyze this dataset, search for relevant supporting evidence online, formulate predictions and forecasts, summarize key insights, and create visualizations.”</em></p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TOOL SEQUENCE &amp; TOKEN USAGE (CLAUDE HAIKU 4.5 – 200K LIMIT)
───────────────────────────────────────────────────────────────────────────────────────────────
STEP / TOOL                 WITHOUT FILTERING                               WITH FILTERING
                            tokens / 200K  (%)  [progress]                  tokens / 200K  (%)  [progress]
───────────────────────────────────────────────────────────────────────────────────────────────

1. Input query              2K / 200K  (1%)   [░░░░░░░░░░░░░░░░░░░░]       2K / 200K  (1%)   [░░░░░░░░░░░░░░░░░░░░]
   "Analyze this            (initial user message)                         (same)
   dataset...

2. read_files               12K / 200K (6%)   [█░░░░░░░░░░░░░░░░░░░░]      12K / 200K (6%)   [█░░░░░░░░░░░░░░░░░░░░]
   (raw data)               (500 rows × 20 cols inline)                    (same data)
   ↳ 500 rows × 20 cols

3. web_search               107K / 200K (54%)[███████████░░░░░░░░░]        16K / 200K (8%)   [██░░░░░░░░░░░░░░░░░░]
   (background refs)        (15 results, full content in context)          (sub-agent summary only, ~2K)
   ↳ 15 results

4. code_analysis            115K / 200K (58%)[████████████░░░░░░░░]        24K / 200K (12%)  [███░░░░░░░░░░░░░░░░░]
   (metrics &amp; stats)        (Python code + long explanation inline)        (more compact reasoning)
   ↳ Python + explainer

5. write_csv                145K / 200K (73%)[███████████████░░░░░]        25K / 200K (13%)  [███░░░░░░░░░░░░░░░░░]
   (aggregated table)       (5K-row CSV stored inline in messages)         (file pointer only; no inline CSV)
   ↳ 5K-row CSV

6. chart_generation         235K / 200K (118%)[████████████████████] ✗     25K / 200K (13%)  [███░░░░░░░░░░░░░░░░░]
   (3 charts)               (3× base64 PNG blobs in context)               (UI gets PNG; agent sees
   ↳ 3× base64 PNG                                                          "[CHART_GENERATED]" stub)

7. Agent summary            — (not reached; overflow at step 6)            26K / 200K (13%)  [███░░░░░░░░░░░░░░░░░] ✓
   "I’ve analyzed your                                                   (final natural-language summary +
   data, here are the                                                    references to charts/table
   key insights and charts…"

───────────────────────────────────────────────────────────────────────────────────────────────
Context limit (Claude Haiku 4.5): 200K tokens

End-to-end latency        ~2–3 minutes (retries on overflow)              ~35 seconds (no retries)

</code></pre></div></div> <p>In this trace, filtering reduced peak context from ~235K to ~26K tokens, an ~89% reduction.</p> <hr> <h2 id="key-takeaways">Key Takeaways</h2> <p>Protect the primary agent by thinking through context on three levels of: agent, user, logs.</p> <ul> <li>Use token count API to visualize exactly how much context is in memory.</li> <li>Be clear on where you are sending stdout from the code execution sandbox.</li> <li>Leverage Sub-LMs to summarize large tool results</li> <li>Apply client and server side filtering.</li> </ul> <hr> <h2 id="references">References</h2> <ul> <li><a href="https://docs.anthropic.com/en/docs/build-with-claude/context-windows" rel="external nofollow noopener" target="_blank">Anthropic: Context Windows</a></li> <li><a href="https://docs.anthropic.com/en/docs/build-with-claude/context-editing" rel="external nofollow noopener" target="_blank">Anthropic: Context Editing</a></li> <li><a href="https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents" rel="external nofollow noopener" target="_blank">Anthropic: Effective Context Engineering for AI Agents</a></li> <li><a href="https://research.trychroma.com/context-rot" rel="external nofollow noopener" target="_blank">Chroma Research: Context Rot</a></li> </ul> <hr> <p><em>Views are strictly my own.</em></p> <p><em>Published: December 8, 2025</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/recursive-lm-code-execution/">Recursive Language Models + Code Execution: 60% accuracy on BrowseComp Plus (no embeddings)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/recursive-language-models/">Recursive Language Models reduce context rot and 2.5× accuracy on BrowseComp‑Plus (at 2.6× latency)</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/anti-patterns-as-guardrails/">Exploring Continuous Learning: Reasoning Bank + Recursive Language Models</a> </li> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>